---
title: "Readability and Topic Model"
author: "Xuyan"
date: "May 12, 2016"
output: html_document
d3: true
---
{% include JB/setup %}

## Readability of the Speeches

We first use the flesch-kincaid grade level to measure the readability of all the speeches.

```{r cars, echo=FALSE, warning=F, message=F}
require('tm')
require('ggplot2')

mypwd = paste(getwd(),"/data",sep = "")

corpora = Corpus(DirSource(mypwd))
# split the documents into individual lists
corpora = Corpus(VectorSource(strsplit(paste(corpora[[1]]$content,collapse='\n'),"***",fixed=TRUE)[[1]]))

# create a list of the president and the time the speech was given

table_content = strsplit(corpora[[1]]$content,split = "\n")[[1]]
index = data.frame(predsident = "",date = "", stringsAsFactors = FALSE)
for(i in 1:length(table_content)){
  if(table_content[i] == "" | table_content[i] == "CONTENTS" ){
    next
  }
  cur = strsplit(trimws(table_content[i]),split = ",")[[1]]
  index = rbind(index,c(cur[1],paste(cur[3],cur[4],sep = " ")))
}
index = index[-1,]


# calculate the readabilities of the speeches
library("koRpus")

fk_score_array = NULL
for(i in 2:length(corpora)){
  write(corpora[[i]]$content,"curText.txt")
  t = tokenize("curText.txt",lang = "en")
  fk_temp = flesch.kincaid(t)
  fk_score_array = c(fk_score_array, fk_temp@Flesch.Kincaid$grade)
}

index$readability = fk_score_array
index$year = as.numeric(substr(index$date,nchar(index$date)-4,nchar(index$date)))

ggplot(index,aes(x = year, y = readability, group=1)) + geom_line() + 
  ggtitle("Readability by Year")
```

The plot shows a trend of decreasing flesch-kincaid grades, which means the speech is becoming easier to understand because the required equivalent grade is decreasing. The next plot shows the readability ranking of all the predidents by the average grades for each president and we can see a preference for easy-to-read texts in the Bush family.

```{r, echo=FALSE, warning=F, message=F}
# readability for each president
pres = unique(index$predsident)
avg_score = NULL

for(pre in pres){
  avg_grade = mean(fk_score_array[index$predsident == pre])
  avg_score = rbind(avg_score,c(pre,avg_grade))
}

avg_score = as.data.frame(avg_score, stringsAsFactors = F)
names(avg_score) = c("president","readability")
avg_score$readability = as.numeric(avg_score$readability)

ggplot(avg_score,aes(x = reorder(president,readability), y = readability))+
  geom_bar(stat = "identity") + coord_flip() + ggtitle("Average Readablity by Presidents") +
  xlab("President")
```

## Topic Model Visualization

After deleting the stop words and steeming the words, we can take a look at the several most frequently used words in the speeches.

```{r, echo=FALSE, warning=F, message=F}
###############
# exploratory #
###############

# preprocessing of data
docs = corpora[2:length(corpora)]
names(docs) =  index$date

docs = tm_map(docs, content_transformer(tolower))
docs = tm_map(docs, removeNumbers)
docs = tm_map(docs, removePunctuation)
docs = tm_map(docs, removeWords, stopwords("english"))
docs = tm_map(docs, stemDocument)
docs = tm_map(docs, stripWhitespace)
docs = tm_map(docs, PlainTextDocument)

myStopwords = c("can", "say","one","way","use",
                  "also","howev","tell","will",
                  "much","need","take","tend","even",
                  "like","particular","rather","said",
                  "get","well","make","ask","come","end",
                  "first","two","help","often","may",
                  "might","see","someth","thing","point",
                  "post","look","right","now","think","'ve ",
                  "'re ","anoth","put","set","new","good",
                  "want","sure","kind","larg","yes,","day","etc",
                  "quit","sinc","attempt","lack","seen","awar",
                  "littl","ever","moreov","though","found","abl",
                  "enough","far","earli","away","achiev","draw",
                  "last","never","brief","bit","entir","brief",
                  "great","lot","must")

docs = tm_map(docs, removeWords, myStopwords)

dtm = DocumentTermMatrix(docs, control = list(minWordLength = 3))

mfreq = colSums(as.matrix(dtm))

p1 = ggplot(subset(data.frame(word=names(mfreq),freq=mfreq),freq>2000),aes(word,freq))+ 
  geom_bar(stat='identity') + 
  theme(axis.text.x=element_text(size=12,color='red',fac='bold.italic',angle=45,hjust=0.5))
p1

```

The we fit a tpoic model with 20 topics, and the result is visualized in the following app. You can play around and see the different words for each topic.

<iframe src='http://xxiao1992.github.io/topicWebsite/vis/#zoom=70' frameborder="no"
width=1500 height=1200 
></iframe>
